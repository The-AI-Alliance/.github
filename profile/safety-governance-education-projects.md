# AI Safety, Governance, and Education

> Collaborate on the necessary enablers of successful AI applications.

In order for the objectives of the [Open Agent Hub](open-agent-hub-projects.md) and the [Open Models and Data Initiative](open-models-and-data-projects.md) to be achieved, fundamental requirements must be met for safety, governance, and the expertise required to use AI technologies effectively.

**AI Safety** encompasses classic cybersecurity, as well as AI-specific concerns, such as suppression of undesirable content and compliance with regulations and social norms. A more general term is _trustworthiness_, which adds concerns about ensuring accuracy (i.e., minimizing hallucinations) and meeting the specific requirements for application use cases, etc. Enterprises won't deploy AI applications into production scenarios if they don't trust them to behave as expected.

**Governance** is an aspect of trustworthiness, specifically the assurances that all end-to-end processes used to create all AI application components are secure, licensed for use, etc. AI models are created with data; they are mostly data themselves. Hence, models, like data,  need to be governed.

Finally, **Education** addresses the needs where organizations struggle to learn all the things they need to know in order to use AI safely and effectively. Not only has AI introduced new tools and techniques to software application development, it has fundamentally altered some of the ways software works, for example, introducing _stochastic_ behaviors as core aspects of application features, where previously _deterministic_ behaviors were the norm. Most AI Alliance projects have dual missions, not only to innovate and create, but to educate.

The following projects address these concerns.

<div class="table-wrapper">
  <table>
    <thead>
      <tr>
        <th>Links</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td class="project-title" colspan="2">
          <a href="https://the-ai-alliance.github.io/trust-safety-user-guide/">The AI Trust and Safety User Guide</a> <img src="https://the-ai-alliance.github.io/assets/images/favicon-16x16.png" alt="AI Alliance icon"/>
        </td>
      </tr>
      <tr>
        <td>
          <ul>
            <li>
              <a href="https://github.com/The-AI-Alliance/trust-safety-user-guide">repo</a>
            </li>
            <li>
              <a href="https://github.com/The-AI-Alliance/trust-safety-user-guide/issues">issues</a>
            </li>
            <li>
              <a href="https://github.com/The-AI-Alliance/trust-safety-user-guide/discussions">discussions</a>
            </li>
          </ul>
        </td>
        <td>
          An introduction to trust and safety concepts from diverse experts, followed by recommendations for how to meet your application's needs. Start here if you are new to trust and safety, then leverage the projects discussed next to implement what you need.
        </td>
      </tr>
      <tr>
        <td class="project-title" colspan="2">
          <a href="https://the-ai-alliance.github.io/ai-application-testing/">Testing Generative AI Applications</a> <img src="https://the-ai-alliance.github.io/assets/images/favicon-16x16.png" alt="AI Alliance icon"/>
        </td>
      </tr>
      <tr>
        <td>
          <ul>
            <li>
              <a href="https://github.com/The-AI-Alliance/ai-application-testing/">repo</a>
            </li>
            <li>
              <a href="https://github.com/orgs/The-AI-Alliance/projects/31">dashboard</a>
            </li>
            <li>
              <a href="https://github.com/The-AI-Alliance/ai-application-testing/issues">issues</a>
            </li>
            <li>
              <a href="https://github.com/The-AI-Alliance/ai-application-testing/discussions">discussions</a>
            </li>
          </ul>
        </td>
        <td>
          Are you an enterprise developer? How should you test AI applications? You know how to write <em>deterministic</em> tests for your "pre-AI" applications. What should you do when you add generative AI models, which aren't deterministic? This project adapts existing evaluation techniques for the "last mile" of AI evaluation; verifying that an AI application correctly implements its requirements and use cases, going beyond the general concerns of evaluation for safety, security, etc. We are building nontrivial, reusable examples and instructional materials, so you can use these techniques effectively in combination with the traditional tools you already know. This project is part of the <a href="https://thealliance.ai/core-projects/trust-and-safety-evaluations" target="tsei">Trust and Safety Evaluation Initiative</a> (TSEI). (It was previously called <em>Achieving Confidence in Enterprise AI Applications</em>.)
        </td>
      </tr>
      <tr>
        <td class="project-title" colspan="2">
          <a href="https://servicenow.github.io/DoomArena/">DoomArena</a>
        </td>
      </tr>
      <tr>
        <td>
          <ul>
            <li>
              <a href="https://arxiv.org/abs/2504.14064">paper</a>
            </li>
            <li>
              <a href="https://www.servicenow.com/blogs/2025/doomarena-security-testing-ai-agents">blog post</a>
            </li>
            <li>
              <a href="https://colab.research.google.com/github/ServiceNow/DoomArena/blob/main/notebooks/doomarena_intro_notebook.ipynb">colab</a>
            </li>
            <li>
              <a href="https://github.com/ServiceNow/DoomArena">repo</a>
            </li>
            <li>
              <a href="https://github.com/ServiceNow/DoomArena/issues">issues</a>
            </li>
          </ul>
        </td>
        <td>
          <p>
            AI agents are becoming increasingly powerful and ubiquitous. They now interact with users, tools, web pages, and databases—each of which introduces potential attack vectors for malicious actors. As a result, the security of AI agents has become a critical concern. <a href="https://servicenow.github.io/DoomArena/">DoomArena</a> provides a modular, configurable framework that enables the simulation of realistic and evolving security threats against AI agents. It helps researchers and developers explore vulnerabilities, test defenses, and improve the security of AI systems. The DoomArena architecture comprises several key components that work together to create a flexible, powerful security testing environment for AI agents:
          </p>
          <p>
            <ul>
              <li><b>Attack Gateway:</b> Functions as a wrapper around original agentic environments (TauBench, BrowserGym, OSWorld), injecting malicious content into the user-agent-environment loop as the AI agent interacts with it.</li>
              <li><b>Threat Model:</b> Defines which components of the agentic framework are attackable and specifies targets for the attacker, enabling fine-grained security testing.</li>
              <li><b>Attack Config:</b> Specifies the AttackableComponent, the AttackChoice (drawn from a library of implemented attacks), and the SuccessFilter which evaluates attack success.</li>
            </ul>
          </p>
          <p>
            DoomArena offers several advanced capabilities that make it a powerful and flexible framework for security testing of AI agents:
          </p>
          <p>
            <ul>
              <li><b>Plug-in:</b> Plug into to your favorite agentic framework and environments τ-Bench, BrowserGym, OSWorld without requiring any modifications to their code.</li>
              <li><b>Customizable threat models:</b> Test agents against various threat models including malicious users and compromised environments.</li>
              <li><b>Generic Attacker Agents:</b> Develop and reuse attacker agents across multiple environments.</li>
              <li><b>Defense Evaluation:</b> Compare effectiveness of guardrail-based, LLM-powered, and security-by-design defenses.</li>
              <li><b>Composable Attacks:</b> Reuse and combine previously published attacks for comprehensive and fine-grained security testing.</li>
              <li><b>Trade-off Analysis:</b> Analyze the utility/security trade-off under various threat models.</li>
            </ul>
          </p>
        </td>
      </tr>      
      <tr>
        <td class="project-title" colspan="2">
          <a href="https://the-ai-alliance.github.io/trust-safety-evals/">Evaluation Is for Everyone</a> <img src="https://the-ai-alliance.github.io/assets/images/favicon-16x16.png" alt="AI Alliance icon"/>
        </td>
      </tr>
      <tr>
        <td>
          <ul>
            <li>
              <a href="https://github.com/The-AI-Alliance/trust-safety-evals">repo</a>
            </li>
            <li>
              <a href="https://github.com/orgs/The-AI-Alliance/projects/23">dashboard</a>
            </li>
            <li>
              <a href="https://github.com/The-AI-Alliance/trust-safety-evals/issues">issues</a>
            </li>
            <li>
              <a href="https://github.com/The-AI-Alliance/trust-safety-evals/discussions">discussions</a>
            </li>
          </ul>
        </td>
        <td>
          <em>Evaluation Is for Everyone</em> addresses two problems: 1) many AI application builders don't know what they should do to ensure trust and safety, and 2) it should be as easy as possible to add trust and safety capabilities to AI applications. Many trust and safety evaluation suites are available that can be executed on the <a href="https://the-ai-alliance.github.io/eval-ref-stack">Evaluation Reference Stack</a>. We are making it as easy as possible for AI application developers to find and deploy the evaluations they need. See also the companion <a href="https://the-ai-alliance.github.io/ai-application-testing/">Testing Generative AI Applications</a> project. This project is part of the <a href="https://thealliance.ai/core-projects/trust-and-safety-evaluations" target="tsei">Trust and Safety Evaluation Initiative</a> (TSEI).
        </td>
      </tr>
      <tr>
        <td class="project-title" colspan="2">
          <a href="https://the-ai-alliance.github.io/eval-ref-stack/">Evaluation Reference Stack</a> <img src="https://the-ai-alliance.github.io/assets/images/favicon-16x16.png" alt="AI Alliance icon"/>
        </td>
      </tr>
      <tr>
        <td>
          <ul>
            <li>
              <a href="https://github.com/The-AI-Alliance/eval-ref-stack/">repo</a>
            </li>
            <li>
              <a href="https://github.com/orgs/The-AI-Alliance/projects/23">dashboard</a>
            </li>
            <li>
              <a href="https://github.com/The-AI-Alliance/eval-ref-stack/issues">issues</a>
            </li>
            <li>
              <a href="https://github.com/The-AI-Alliance/eval-ref-stack/discussions">discussions</a>
            </li>
          </ul>
        </td>
        <td>
          The companion projects <a href="https://the-ai-alliance.github.io/ai-application-testing/">Testing Generative AI Applications</a> and <a href="https://the-ai-alliance.github.io/trust-safety-evals/">Evaluation Is for Everyone</a> require a runtime stack that is flexible and easy to deploy and manage. This project is collating popular tools for writing and running evaluations into easy-to-consume packages. This project is part of the <a href="https://thealliance.ai/core-projects/trust-and-safety-evaluations" target="tsei">Trust and Safety Evaluation Initiative</a> (TSEI).
        </td>
      </tr>
      <!--
      <tr>
        <td class="project-title" colspan="2">
          <a href="https://the-ai-alliance.github.io/ranking-safety-priorities/">Ranking AI Safety Priorities by Domain</a> <img src="https://the-ai-alliance.github.io/assets/images/favicon-16x16.png" alt="AI Alliance icon"/>
        </td>
      </tr>
      <tr>
        <td>
          <ul>
            <li>
              <a href="https://github.com/The-AI-Alliance/ranking-safety-priorities">repo</a>
            </li>
            <li>
              <a href="https://github.com/orgs/The-AI-Alliance/projects/32">dashboard</a>
            </li>
            <li>
              <a href="https://github.com/The-AI-Alliance/ranking-safety-priorities/issues">issues</a>
            </li>
            <li>
              <a href="https://github.com/The-AI-Alliance/ranking-safety-priorities/discussions">discussions</a>
            </li>
          </ul>
        </td>
        <td>
          What are the most important safety concerns for your specific domain and use cases? This project explores these questions in several industries, healthcare, finance, education, and legal, with more to come.
        </td>
      </tr>
      -->
      <tr>
        <td class="project-title" colspan="2">
          <a href="https://www.unitxt.ai/">unitxt</a>
        </td>
      </tr>
      <tr>
        <td>
          <ul>
            <li>
              <a href="https://github.com/IBM/unitxt">repo</a>
            </li>
            <li>
              <a href="https://github.com/IBM/unitxt/issues">issues</a>
            </li>
            <li>
              <a href="https://github.com/IBM/unitxt/discussions">discussions</a>
            </li>
          </ul>
        </td>
        <td>
           Unitxt is a Python library for enterprise-grade evaluation of AI performance, offering the world's largest catalog of tools and data for end-to-end AI benchmarking. (Principal developer: <a href="https://research.ibm.com/">IBM Research</a>)
        </td>
      </tr>
    </tbody>
  </table>
</div>
